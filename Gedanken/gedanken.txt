Kodierung vs Aktionen:

Eine Kodierung liegt in diesem Kontext vor, wenn mehrere Ausgaben als eine Einheit betrachtet werden weil diese nur als Ganzes ein vollständiges Ergebnis darstellen.

Beispiel aktion Zugauswahl Vier Gewinnt: 0 0 0 1 0 0 0  => Jede spalte hat eine Bewertung, es wird spalte 3 gewählt (indexierung beginnt bei 0), weil diese am besten bewertet ist.

Beispiel Zugauswahl über kodierung: 0 0 1 1 => Binärzahl Spalte 3, der Wert ist nicht bekannt aber Spalte 3 wird gewählt.

Vorteile von Aktionen:
Aktionen können nach ihrem erreichten Erfolg/Misserfolg bewertet werden und Kodierungen nicht.
=> bei der Aktion kristallisiert sich numerisch heraus, wie erfolgsversprechend diese ist.

Nachteile von Aktionen:
für jede Aktion ist ein eigenes Ausgabeneuron erforderlich.
=> Mit steigender Aktionsraumkomplexität wächst auch die Liste der möglichen Aktionen linear an.
=> Zahl der Aktionen wird zu groß um diese mit einer Ausgabe zu bewältigen
=> Zusätzliche neuronale Netze erforderlich um die Aktion unter Angabe vorangegangener Entscheidungen zu präzisieren. Beispiel Schach: Startfeldnetz: 64Startfelder, Zielfeldnetz: 64Zielfelder + 64Umwandlungen

Vorteile einer Kodierung:
Mit steigender Aktionsraumkomplexität wächst die Kodierung nur logarithmisch (2erlogarithmus bei binärkodierung)
=> Deutlich mehr Information kann mit einer Entscheidung übermittelt werden.
Beispiel Schach: StartUndZielfeldNetz: (Binärkodierung: 8Neuronen Startfeld + 8 Neuronen Zielfeld + 2 Neuronen Umwandlung) = 18 Neuronen für eine komplette Zugangabe.
Wäre es Ziel in einem Model einzelne Aktionen als Züge darzustellen, dann wären 64 * (2 * 64) = 8192 Ausgabeneuronen erforderlich.

Nachteil einer Kodierung:
- Das lernen über Kodierungen könnte mehr Daten erfordern, weil schlechte entscheoidungen jeweils kein negatives feedback erzeugen (mit bewertung ist immer ein feedback für gute UND schlechte aktionen möglich)
- Die möglichen Aktionen können nicht relativ zueinander bewertet werden, weil alle Aktionen in einer Kodierung zusammengemischt sind!

Kodierungen die zum Erfolg geführt haben können aber stattdessen einfach als Richtig angenommen werden anstatt diesen eine konkrete Bewertung zuzuweisen!

Beispiel: Wenn das Netz die Kodierung 0.6, 0.3, 1.0 liefert, und diese Kodierung führt zu einem besseren Ergebnis als eine andere, dann wird das Netz für diese Eingabe mit 1.0, 0.0, 1.0 trainiert. Es kann auch sein, dass durch Zufallseinfluss eine Kodierung gewählt wird, die nicht der Zielkodierung des Netzes entspricht diese aber zu einem besseren Ergebnis führt, dann setzt sich stattdessen diese andere Kodierung durch. => Es ist wichtig zu erkennen, dass als Ergebnis keine nutzlose Vermischung aus allen Kodierungen rauskommt, die irgendwie sinnvoll sind, wäre das nämlich so, dann würden Autoencoder für Bilder garnicht funktionieren, denn auch das Bild ist jeweils eine von vielen möglichen Kodierungen => dennoch kommt kein Matsch heraus.


Neuronales netz input für Rechtschreibkorrektur:

Altinformation: 512Bit = 64 Zeichen //was waren die letzten 64 zeichen aus dem zu korrigierenden text?
Neuinformation: 768Bit = 96 Zeichen //was ist das neu generierte was links von der aktuell zu korrigierenden stelle steht?
Aktuellinformation: 8Bit = 1 Zeichen //die zu korrigierende oder zu übernehmende stelle
Fortsetzungsinformation 760Bit = 95 Zeichen //wie geht der bisher fehlerhafte Text innerhalb des Schiebefensters weiter?
=> Macht 512 + 768 + 8 + 760 = 2048 Eingabeneuronen

00000000 => keinzeichen
11111111 => unbekanntes zeichen wird übersprungen und einfach übernommen

Output für Rechtschreibkorrektur:

1 Neuron für (Ersetzen oder Einfügen)
8 weitere Neuronen, für die Kodierung eines 8 bit langen zeichens, welches entweder zur einfügung oder ersetzung oder löschung dient => ein zeichen bleibt unverändert, wenn es durch sich selbst ersetzt wird, ein Zeichen wird gelöscht, wenn es durch null ersetzt wird

Fehlergenerator Fähigkeiten (generiert aus fehlerfreien Text, fehlerbehafteten Text, welcher zu korrigieren ist):
- Einfügen und entfernen einzelner zeichen
- Zufallszeichenersetzung
- überflüssige kommas
- entfernte kommas
- buchstabenverdreher
- der die das fehler
- daß das dass fehler
- großkleinschreibung fehler
- deppenleerzeichen
- einzahl mehrzahl fehler
- einbau uneinheitlicher anredeformen (Du, Sie, Ihr, Ihnen etc.)
- doppelte buchstaben oder entfernen doppelter
- verwechslung und vertauschung ähnlicher buchstaben z.B. dt td bei Stadt und Statd oder Stat oder Statt
- verwechslung mit ähnlichen Wörtern
- vertauschung von Wörtern
- zusammenmischung von Wörtern z.b. Wat masndu da? => Was machst du denn da?

Modeloptimierung:
- Wahrscheinlich ähnliche Strategie wie bei der Zahlenaddition möglich. (Fehlerkorrekturmodelle für vorangegangene Modelle)

1. Ein zeichen zusätzlich im ErrorText entspricht einer hinzufügung
2. Ein zeichen zuwenig einer Deletion
3. Alles andere sind Ersetzungen.
4. Wird in der nachfolgenden Iteration einer ersetzung ersetzt, so blreibt es eine Ersetzung.
5. Wird eine ersetzung entfernt so wird es eine Deletion
6. Wird eine hinzufügung entfernt so wird das element der operation entfernt.


LAYER SIZES:
2048
3072
2048
2048
1536
1024
1024
512
256
9